{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine_tuning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xv-c-EvalfLp"},"source":["## Installing libraries, loading weights, config and vocab files\n","\n","Here we install our Tapas implementation, as well as the `torch-scatter` dependency library."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76-hgr7Z1eq0","executionInfo":{"status":"ok","timestamp":1620178390084,"user_tz":240,"elapsed":20098,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"af896171-2014-44e7-8356-e6f38ebe6ea4"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","import sys\n","import os\n","import numpy as np\n","# from transformers import TapasTokenizer, TapasForQuestionAnswering,BertTokenizer,TapasConfig, AdamW\n","\n","prefix = '/content/gdrive/My Drive/'\n","# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n","customized_path_to_your_homework = 'VT-1/Intro DL/Final Project/My Exp'\n","sys_path = os.path.join(prefix, customized_path_to_your_homework)\n","sys.path.append(sys_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jjDWQyoDlRxt","executionInfo":{"status":"ok","timestamp":1620178413312,"user_tz":240,"elapsed":43294,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"5480527f-c0b5-45d7-890c-21274eb5cebe"},"source":["! rm -r transformers\n","! git clone -b tapas_v4_debugging_backward_pass https://github.com/NielsRogge/transformers.git\n","! cd transformers\n","! pip install ./transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["rm: cannot remove 'transformers': No such file or directory\n","Cloning into 'transformers'...\n","remote: Enumerating objects: 71336, done.\u001b[K\n","remote: Counting objects: 100% (1563/1563), done.\u001b[K\n","remote: Compressing objects: 100% (468/468), done.\u001b[K\n","remote: Total 71336 (delta 962), reused 1411 (delta 874), pack-reused 69773\u001b[K\n","Receiving objects: 100% (71336/71336), 53.89 MiB | 23.04 MiB/s, done.\n","Resolving deltas: 100% (50318/50318), done.\n","Processing ./transformers\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.0.dev0) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.0.dev0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.0.dev0) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.0.dev0) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.0.dev0) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.0.dev0) (20.9)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 5.1MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 43.8MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.1.0.dev0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.1.0.dev0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.1.0.dev0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.1.0.dev0) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.1.0.dev0) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.1.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.1.0.dev0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.1.0.dev0) (1.0.1)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.1.0.dev0-cp37-none-any.whl size=1500349 sha256=c09ac43aa58f139b7782506c9b56e7c4fe26843c85d4b76cb48101cb6cc649e9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xj65nouq/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n","Successfully built transformers\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.1.0.dev0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cwf2iPEblbox","executionInfo":{"status":"ok","timestamp":1620178417394,"user_tz":240,"elapsed":47356,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"4d1af1dc-2a3f-493d-83c8-3aebae947ea1"},"source":["!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","Collecting torch-scatter\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.8.0%2Bcu101/torch_scatter-2.0.6-cp37-cp37m-linux_x86_64.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 2.5MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0avgCPHYlhvk"},"source":["## Preparing the data for the model using TapasTokenizer\n","\n","Let's read in a collection of 10 table-question pairs from the WTQ test set, on which we will further fine-tune TAPAS."]},{"cell_type":"code","metadata":{"id":"R8FFqXzCmNh0","colab":{"base_uri":"https://localhost:8080/","height":78},"executionInfo":{"status":"ok","timestamp":1620180095661,"user_tz":240,"elapsed":370,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"8a6ab394-1195-4f45-db47-319de9712a71"},"source":["import pandas as pd\n","\n","data = pd.read_excel(sys_path+\"/Data/Test_5.xlsx\")\n","data=data[24:]\n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>annotator</th>\n","      <th>position</th>\n","      <th>question</th>\n","      <th>table_file</th>\n","      <th>answer_coordinates</th>\n","      <th>answer_text</th>\n","      <th>aggregation</th>\n","      <th>float_answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>24</th>\n","      <td>nu-21</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Number of patients in ICU currently in Virginia?</td>\n","      <td>All_States.csv</td>\n","      <td>['(6,2)']</td>\n","      <td>['258']</td>\n","      <td>COUNT</td>\n","      <td>258.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  annotator  position  ... answer_text aggregation float_answer\n","24  nu-21        0.0       0.0  ...     ['258']       COUNT        258.0\n","\n","[1 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"rnm4XtrsvCox"},"source":["# data_NC = pd.read_csv(sys_path+\"/Data/NC.csv\")\n","# data_NC=data_NC[:3]\n","# data_NY = pd.read_csv(sys_path+\"/Data/NY.csv\")\n","# data_NY=data_NY[:3]\n","# data_VA = pd.read_csv(sys_path+\"/Data/VA.csv\")\n","# data_VA=data_VA[:3]\n","# # data_US = pd.read_csv(sys_path+\"/Data/US.csv\")\n","# # data_US=data_US[:3]\n","# df_all=pd.concat([data_NC,data_NY,data_VA])\n","# df_all.to_csv(sys_path+'/Data/All_States.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFYZtW9qoAQJ"},"source":["Here we make sure that the `answer_coordinates` and `answer_text` columns are converted into true Python lists of tuples/strings respectively."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":78},"id":"OzvG5IJAnUmQ","executionInfo":{"status":"ok","timestamp":1620180095904,"user_tz":240,"elapsed":578,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"a4560b1f-a60a-446c-b976-1ecd23a86cc2"},"source":["import ast\n","\n","def _parse_answer_coordinates(answer_coordinate_str):\n","  \"\"\"Parses the answer_coordinates of a question.\n","  Args:\n","    answer_coordinate_str: A string representation of a Python list of tuple\n","      strings.\n","      For example: \"['(1, 4)','(1, 3)', ...]\"\n","  \"\"\"\n","\n","  try:\n","    answer_coordinates = []\n","    # make a list of strings\n","    coords = ast.literal_eval(answer_coordinate_str)\n","    # parse each string as a tuple\n","    for row_index, column_index in sorted(\n","        ast.literal_eval(coord) for coord in coords):\n","      answer_coordinates.append((row_index, column_index))\n","  except SyntaxError:\n","    raise ValueError('Unable to evaluate %s' % answer_coordinate_str)\n","  \n","  return answer_coordinates\n","\n","\n","def _parse_answer_text(answer_text):\n","  \"\"\"Populates the answer_texts field of `answer` by parsing `answer_text`.\n","  Args:\n","    answer_text: A string representation of a Python list of strings.\n","      For example: \"[u'test', u'hello', ...]\"\n","    answer: an Answer object.\n","  \"\"\"\n","  try:\n","    answer = []\n","    for value in ast.literal_eval(answer_text):\n","      answer.append(value)\n","  except SyntaxError:\n","    raise ValueError('Unable to evaluate %s' % answer_text)\n","\n","  return answer\n","\n","data['answer_coordinates'] = data['answer_coordinates'].apply(lambda coords_str: _parse_answer_coordinates(coords_str))\n","data['answer_text'] = data['answer_text'].apply(lambda txt: _parse_answer_text(txt))\n","\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>annotator</th>\n","      <th>position</th>\n","      <th>question</th>\n","      <th>table_file</th>\n","      <th>answer_coordinates</th>\n","      <th>answer_text</th>\n","      <th>aggregation</th>\n","      <th>float_answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>24</th>\n","      <td>nu-21</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Number of patients in ICU currently in Virginia?</td>\n","      <td>All_States.csv</td>\n","      <td>[(6, 2)]</td>\n","      <td>[258]</td>\n","      <td>COUNT</td>\n","      <td>258.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  annotator  position  ... answer_text aggregation float_answer\n","24  nu-21        0.0       0.0  ...       [258]       COUNT        258.0\n","\n","[1 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"ZBJnrIxStF1N"},"source":["Next, we initialize the tokenizer, which can be used to prepare the data for the model."]},{"cell_type":"code","metadata":{"id":"bH6XKHtglkgH"},"source":["from transformers import TapasTokenizer\n","\n","tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e4VAk_KQsvA8"},"source":["Here we create a PyTorch dataset and corresponding dataloader. We encode each table-question pair independently using the tokenizer."]},{"cell_type":"code","metadata":{"id":"6hoxvp9Bl9ZR"},"source":["import torch\n","\n","table_csv_path = sys_path+'/Data/'\n","\n","class TableDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","    def __getitem__(self, idx):\n","        item = data.iloc[idx]\n","        table = pd.read_csv(table_csv_path + item.table_file).astype(str)\n","        cols=['state','death','inIcuCurrently','hospitalizedCurrently']\n","        table=table[cols][:9]\n","        encoding = self.tokenizer(table=table, \n","                                  queries=item.question, \n","                                  answer_coordinates=item.answer_coordinates, \n","                                  answer_text=item.answer_text,\n","                                  padding=\"max_length\",\n","                                  truncation=True,\n","                                  return_tensors=\"pt\"\n","        )\n","        # remove the batch dimension which the tokenizer adds \n","        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n","        # add float answer (weak supervision for aggregation)\n","        encoding[\"float_answer\"] = torch.tensor(item.float_answer)\n","        return encoding\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","train_dataset = TableDataset(data, tokenizer)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbPbnpbf7JFR","executionInfo":{"status":"ok","timestamp":1620180096555,"user_tz":240,"elapsed":1200,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"828bb64c-0141-4e13-d8f9-23e7a61a48ab"},"source":["print(data.table_file)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["24    All_States.csv\n","Name: table_file, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxhmZ8ba-LJb","executionInfo":{"status":"ok","timestamp":1620180169524,"user_tz":240,"elapsed":454,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"9775558e-c290-491d-b795-8d1f4968d2e8"},"source":["table = pd.read_csv(table_csv_path + data.table_file[24]).astype(str)\n","print(table.columns)\n","print(data.table_file[24])\n","print(data.question[24])\n","cols=['state','death','inIcuCurrently','hospitalizedCurrently']\n","\n","table=table[cols][:9]\n","print(table)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['Unnamed: 0', 'date', 'state', 'death', 'deathConfirmed',\n","       'deathIncrease', 'deathProbable', 'hospitalized',\n","       'hospitalizedCumulative', 'hospitalizedCurrently',\n","       'hospitalizedIncrease', 'inIcuCumulative', 'inIcuCurrently', 'negative',\n","       'negativeIncrease', 'negativeTestsAntibody',\n","       'negativeTestsPeopleAntibody', 'negativeTestsViral',\n","       'onVentilatorCumulative', 'onVentilatorCurrently', 'positive',\n","       'positiveCasesViral', 'positiveIncrease', 'positiveScore',\n","       'positiveTestsAntibody', 'positiveTestsAntigen',\n","       'positiveTestsPeopleAntibody', 'positiveTestsPeopleAntigen',\n","       'positiveTestsViral', 'recovered', 'totalTestEncountersViral',\n","       'totalTestEncountersViralIncrease', 'totalTestResults',\n","       'totalTestResultsIncrease', 'totalTestsAntibody', 'totalTestsAntigen',\n","       'totalTestsPeopleAntibody', 'totalTestsPeopleAntigen',\n","       'totalTestsPeopleViral', 'totalTestsPeopleViralIncrease',\n","       'totalTestsViral', 'totalTestsViralIncrease'],\n","      dtype='object')\n","All_States.csv\n","Number of patients in ICU currently in Virginia?\n","  state    death inIcuCurrently hospitalizedCurrently\n","0    NC  11502.0          309.0                1179.0\n","1    NC  11502.0          309.0                1179.0\n","2    NC  11446.0          314.0                1226.0\n","3    NY  39029.0          999.0                4789.0\n","4    NY  38970.0         1012.0                4954.0\n","5    NY  38891.0         1030.0                5034.0\n","6    VA   9596.0          258.0                1127.0\n","7    VA   9519.0          263.0                1164.0\n","8    VA   9428.0          254.0                1222.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FU4bDNgt-edm"},"source":["# ['hospitalizedCurrently','inIcuCurrently','death']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kF_BtIv7iv2p","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1620180096760,"user_tz":240,"elapsed":1374,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"eda290cf-b3db-4040-a387-ad00e5c77ce0"},"source":["tokenizer.decode(train_dataset[0][\"input_ids\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[CLS] number of patients in icu currently in virginia? [SEP] state death inicucurrently hospitalizedcurrently nc 11502. 0 309. 0 1179. 0 nc 11502. 0 309. 0 1179. 0 nc 11446. 0 314. 0 1226. 0 ny 39029. 0 999. 0 4789. 0 ny 38970. 0 1012. 0 4954. 0 ny 38891. 0 1030. 0 5034. 0 va 9596. 0 258. 0 1127. 0 va 9519. 0 263. 0 1164. 0 va 9428. 0 254. 0 1222. 0 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"markdown","metadata":{"id":"aGpphZu1oJsu"},"source":["## Fine-tuning TapasForQuestionAnswering"]},{"cell_type":"markdown","metadata":{"id":"M1MwCPCho3wS"},"source":["We can start from the already fine-tuned checkpoint:"]},{"cell_type":"code","metadata":{"id":"0n6MJyMqyC0N"},"source":["from transformers import TapasForQuestionAnswering, AdamW\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\",\n","                                                  answer_loss_cutoff=None)\n","                                                  \n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=0.0000193581)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owu1_dkcc_0b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620180100909,"user_tz":240,"elapsed":5501,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"6ccaa625-62f4-4826-97a9-2094c2235e2f"},"source":["batch = next(iter(train_dataloader))\n","batch[\"input_ids\"].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 512])"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"code","metadata":{"id":"behCQI0QoIOr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620180164706,"user_tz":240,"elapsed":69288,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"3f6e8755-868e-471c-aeee-4759355466e6"},"source":["for epoch in range(10):  # loop over the dataset multiple times\n","    print(\"-------------\")\n","    print(\"Epoch:\", epoch)\n","    for idx, batch in enumerate(train_dataloader):\n","         print('Example:', idx)\n","         # get the inputs;\n","         input_ids = batch['input_ids'].to(device)\n","         attention_mask = batch['attention_mask'].to(device)\n","         token_type_ids = batch['token_type_ids'].to(device)\n","         labels = batch['labels'].to(device)\n","         numeric_values = batch['numeric_values'].to(device)\n","         numeric_values_scale = batch['numeric_values_scale'].to(device)\n","         float_answer = batch['float_answer'].to(device)\n","\n","         #print(label_ids.size())\n","         \n","         # zero the parameter gradients\n","         optimizer.zero_grad()\n","\n","         # forward + backward + optimize\n","         outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n","                        labels=labels, numeric_values=numeric_values, numeric_values_scale=numeric_values_scale,\n","                        float_answer=float_answer)\n","         loss = outputs.loss\n","         print(f\"Loss: {loss.item()}\")\n","         loss.backward()\n","\n","        #  print(model.column_output_weights.grad) \n","        #  print(model.aggregation_classifier.weight.grad)\n","        #  print(model.aggregation_classifier.bias.grad)\n","\n","         optimizer.step()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-------------\n","Epoch: 0\n","Example: 0\n","Selection loss per example:\n","tensor([2.4584], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([254.0000], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.4774], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.47743016481399536\n","-------------\n","Epoch: 1\n","Example: 0\n","Selection loss per example:\n","tensor([2.8765], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([266.7360], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([1.0514], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 1.051405906677246\n","-------------\n","Epoch: 2\n","Example: 0\n","Selection loss per example:\n","tensor([3.7015], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n","-------------\n","Epoch: 3\n","Example: 0\n","Selection loss per example:\n","tensor([4.9087], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n","-------------\n","Epoch: 4\n","Example: 0\n","Selection loss per example:\n","tensor([6.1212], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n","-------------\n","Epoch: 5\n","Example: 0\n","Selection loss per example:\n","tensor([7.0176], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n","-------------\n","Epoch: 6\n","Example: 0\n","Selection loss per example:\n","tensor([7.7114], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n","-------------\n","Epoch: 7\n","Example: 0\n","Selection loss per example:\n","tensor([8.2827], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n","-------------\n","Epoch: 8\n","Example: 0\n","Selection loss per example:\n","tensor([8.7722], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n","-------------\n","Epoch: 9\n","Example: 0\n","Selection loss per example:\n","tensor([9.2012], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n","Loss: 0.03305523842573166\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wwSFSDDYChfS"},"source":["## Inference\n","\n","Let's investigate the predictions on the examples on which we just fine-tuned the model (from the WTQ test set)."]},{"cell_type":"code","metadata":{"id":"eGoKaFXQ4A8L"},"source":["item = data.iloc[0]\n","table = pd.read_csv(table_csv_path + item.table_file).astype(str)\n","cols=['state','death','inIcuCurrently','hospitalizedCurrently']\n","\n","table=table[cols][:9]\n","encoding = tokenizer(table=table, \n","                          queries=item.question, \n","                          answer_coordinates=item.answer_coordinates, \n","                          answer_text=item.answer_text,\n","                          truncation=True,\n","                          padding=\"max_length\",\n","                          return_tensors=\"pt\"\n",")\n","encoding[\"float_answer\"] = torch.tensor(item.float_answer).unsqueeze(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y0YzoGY24I0C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620180166697,"user_tz":240,"elapsed":71258,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"6b7970ea-669a-4b52-a97b-5483d5226af7"},"source":["encoding = {k: v.to(device) for k,v in encoding.items()}\n","outputs = model(**encoding)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selection loss per example:\n","tensor([9.5816], grad_fn=<AddBackward0>)\n","Expected result:\n","tensor([258.3333], grad_fn=<SumBackward1>)\n","Per example answer loss scaled:\n","tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)\n","Large answer loss mask:\n","tensor([1.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"86th7fEl4VbE"},"source":["predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(encoding, outputs.logits, outputs.logits_aggregation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoPwejPr4gyA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620180166699,"user_tz":240,"elapsed":71239,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"1b8375f1-5ac2-4750-fdf1-2e7dab6de82e"},"source":["predicted_answer_coordinates"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[(6, 2), (7, 2), (8, 2)]]"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"CoAutUVW4paV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620180166700,"user_tz":240,"elapsed":71228,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"ce76c5bd-a6e7-4ab1-aef0-eff5b56a025e"},"source":["predicted_aggregation_indices"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2]"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"markdown","metadata":{"id":"vm-Lp4Tm_p0R"},"source":["We can also do inference all at once:"]},{"cell_type":"code","metadata":{"id":"JfXgmZ-d5sf_","colab":{"base_uri":"https://localhost:8080/","height":457},"executionInfo":{"status":"ok","timestamp":1620180169022,"user_tz":240,"elapsed":73537,"user":{"displayName":"Shiva Maruth Alapati","photoUrl":"","userId":"04293424226706752074"}},"outputId":"cc4e3479-6a72-443d-bfcd-8e8ce04bf05f"},"source":["for i in range(len(train_dataset)):  \n","  item = data.iloc[i]\n","  table = pd.read_csv(table_csv_path + item.table_file).astype(str)\n","  cols=['state','death','inIcuCurrently','hospitalizedCurrently']\n","\n","  table=table[cols][:9]\n","  encoding = tokenizer(table=table, \n","                            queries=item.question, \n","                            truncation=True,\n","                            padding=\"max_length\",\n","                            return_tensors=\"pt\"\n","  )\n","  encoding = {k: v.to(device) for k,v in encoding.items()}\n","  # forward pass to get the logits\n","  outputs = model(**encoding)\n","  # use TapasTokenizer's function to convert them to predicted answer coordinates and aggregation indices\n","  predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(encoding, outputs.logits, \n","                                                                                                        outputs.logits_aggregation)\n","  aggregation2idx = {0: 'NONE', 1:'SUM', 2:'AVERAGE', 3:'COUNT'}\n","\n","  # print the result!\n","  display(table)\n","  print(\"\")\n","  print(item.question)\n","  print(f\"Predicted aggregation: {aggregation2idx[predicted_aggregation_indices[0]]}\")\n","  print(\"Predicted cell values:\")\n","  answers = ', '.join([table.iat[coord] for coord in predicted_answer_coordinates[0]])\n","  print(answers)\n","  print(\"------\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selection loss per example:\n","tensor([10016.5762], grad_fn=<AddBackward0>)\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>state</th>\n","      <th>death</th>\n","      <th>inIcuCurrently</th>\n","      <th>hospitalizedCurrently</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NC</td>\n","      <td>11502.0</td>\n","      <td>309.0</td>\n","      <td>1179.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NC</td>\n","      <td>11502.0</td>\n","      <td>309.0</td>\n","      <td>1179.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NC</td>\n","      <td>11446.0</td>\n","      <td>314.0</td>\n","      <td>1226.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NY</td>\n","      <td>39029.0</td>\n","      <td>999.0</td>\n","      <td>4789.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NY</td>\n","      <td>38970.0</td>\n","      <td>1012.0</td>\n","      <td>4954.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>NY</td>\n","      <td>38891.0</td>\n","      <td>1030.0</td>\n","      <td>5034.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>VA</td>\n","      <td>9596.0</td>\n","      <td>258.0</td>\n","      <td>1127.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>VA</td>\n","      <td>9519.0</td>\n","      <td>263.0</td>\n","      <td>1164.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>VA</td>\n","      <td>9428.0</td>\n","      <td>254.0</td>\n","      <td>1222.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  state    death inIcuCurrently hospitalizedCurrently\n","0    NC  11502.0          309.0                1179.0\n","1    NC  11502.0          309.0                1179.0\n","2    NC  11446.0          314.0                1226.0\n","3    NY  39029.0          999.0                4789.0\n","4    NY  38970.0         1012.0                4954.0\n","5    NY  38891.0         1030.0                5034.0\n","6    VA   9596.0          258.0                1127.0\n","7    VA   9519.0          263.0                1164.0\n","8    VA   9428.0          254.0                1222.0"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Number of patients in ICU currently in Virginia?\n","Predicted aggregation: AVERAGE\n","Predicted cell values:\n","258.0, 263.0, 254.0\n","------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8kJ6qpFs9tq0"},"source":[""],"execution_count":null,"outputs":[]}]}